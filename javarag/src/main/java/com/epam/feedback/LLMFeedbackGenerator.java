package com.epam.feedback;

import com.epam.model.AnalysisFinding;
import com.epam.model.KnowledgeEntry;

/**
 * DEMO: Shows how LLM-based generation would differ from template-based generation.
 * This is a MOCK implementation to illustrate the concept without external dependencies.
 * 
 * In production, replace with actual LLM client (OpenAI, Anthropic, Ollama, etc.)
 */
public class LLMFeedbackGenerator {
    
    /**
     * Simulates LLM-based feedback generation.
     * In production, this would call an actual LLM API.
     * 
     * @param finding The code issue detected
     * @param entry Knowledge base context
     * @param codeSnippet The actual code snippet (optional)
     * @return Generated feedback (currently simulated)
     */
    public String generateFeedback(
        AnalysisFinding finding, 
        KnowledgeEntry entry,
        String codeSnippet
    ) {
        // DEMO: This simulates what an LLM would do
        // In production, replace with:
        // return openAIClient.chat(buildPrompt(finding, entry, codeSnippet));
        
        return simulateLLMResponse(finding, entry, codeSnippet);
    }
    
    /**
     * Simulates LLM response to show the difference from template-based generation.
     * Notice how this can:
     * - Adapt tone based on severity
     * - Provide contextual explanations
     * - Reference specific code patterns
     * - Suggest multiple alternatives
     */
    private String simulateLLMResponse(
        AnalysisFinding finding,
        KnowledgeEntry entry,
        String codeSnippet
    ) {
        // This is what the prompt to an LLM would look like:
        String prompt = buildPrompt(finding, entry, codeSnippet);
        
        // Simulated LLM response (in production, this comes from GPT-4, Claude, etc.)
        return String.format("""
            === LLM-GENERATED FEEDBACK (SIMULATED) ===
            
            I noticed you're using %s in your code. Let me explain why this might be problematic:
            
            %s
            
            Here's what I recommend:
            %s
            
            This is particularly important because modern Java applications benefit from 
            non-synchronized collections in single-threaded contexts, which is the common case.
            
            Would you like me to show you how to refactor this code?
            
            Reference: %s
            ==========================================
            
            [DEMO NOTE: In production, this would be generated by an actual LLM]
            [DEMO NOTE: The prompt sent would be:]
            %s
            """,
            finding.issue(),
            entry.getDescription(),
            entry.getExample(),
            entry.getReference(),
            prompt
        );
    }
    
    /**
     * Builds the prompt that would be sent to an LLM.
     * This demonstrates prompt engineering for RAG.
     */
    private String buildPrompt(
        AnalysisFinding finding,
        KnowledgeEntry entry,
        String codeSnippet
    ) {
        return String.format("""
            You are an expert Java code reviewer. Analyze this code issue and provide helpful feedback.
            
            ISSUE DETECTED:
            %s
            
            LOCATION:
            %s
            
            KNOWLEDGE BASE CONTEXT:
            Title: %s
            Type: %s
            Description: %s
            Example: %s
            
            CODE SNIPPET:
            %s
            
            Provide a friendly, educational explanation that helps the developer understand:
            1. Why this is an issue
            2. What the better alternative is
            3. When this pattern might be acceptable (if ever)
            
            Keep the tone conversational and encouraging.
            """,
            finding.issue(),
            finding.details(),
            entry.getTitle(),
            entry.getType(),
            entry.getDescription(),
            entry.getExample(),
            codeSnippet != null ? codeSnippet : "N/A"
        );
    }
    
    /**
     * PRODUCTION EXAMPLE: How to integrate with a real LLM
     * Uncomment and add dependencies to use:
     * 
     * <dependency>
     *   <groupId>dev.langchain4j</groupId>
     *   <artifactId>langchain4j</artifactId>
     *   <version>0.27.0</version>
     * </dependency>
     */
    /*
    private String callRealLLM(String prompt) {
        // Option 1: OpenAI
        ChatLanguageModel model = OpenAiChatModel.builder()
            .apiKey(System.getenv("OPENAI_API_KEY"))
            .modelName("gpt-4")
            .build();
        
        // Option 2: Local Ollama (free, no API key needed)
        ChatLanguageModel model = OllamaChatModel.builder()
            .baseUrl("http://localhost:11434")
            .modelName("llama2")
            .build();
        
        return model.generate(prompt);
    }
    */
}
